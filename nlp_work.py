# -*- coding: utf-8 -*-
"""NLP_work

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_CqXuBC7ot5GGj4UccfwEnYSs25oB8ti
"""

import pandas as pd
import re

data = pd.DataFrame({"text_data" :  ["hello world", "python is amazing %@#" ,"text manipulation tutorial is on YT 123","Let's learn python"]})

data["length"] = data["text_data"].apply(len)
data["uppercase"] = data["text_data"].str.upper()

data

"""ÖZEL KARAKTERLERİ KALDIRMA


"""

data['Cleaned_data']=data['text_data'].apply(lambda x:re.sub(r"[^A-Za-z0-9\s]","" , x))

data

"""HARFLERİN HEPSİNİ KÜÇÜK YAPMA

"""

data['Cleaned_data'] = data['Cleaned_data'].apply(lambda x: x.lower())

data

"""sayıları kaldırma

"""

data['Cleaned_data'] = data['Cleaned_data'].apply(lambda x: re.sub(r"\d", "",x))

data['Cleaned_data'] = data['Cleaned_data']

df = data

df

data['Cleaned_data'] = data['Cleaned_data'].apply(lambda x: re.sub(r"\s+", " ",x).strip())
df

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from nltk.stem import PorterStemmer

dataset = ["Tokenization is an essential step in NLP",
           "Stemming reduces words to their root form",
           "Natural Language Processing"]

tokenized_sentences = [word_tokenize(sentence) for sentence in dataset]
tokenized_sentences

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from nltk import pos_tag

sentence = "NLP is amazing!"

words = word_tokenize(sentence)
words

"""pos_tag kelimelerin kodlarını getirir bize . Kodlar da kendi wp mesajında var !


"""

pos_tags = pos_tag(words)
pos_tags

import pandas as pd

data  = {"Sentences" : ["NLTK is powerful tool for NLP","Turkey is a great country" , "Reject Modernity , Embrace Masculinity"]}

df = pd.DataFrame(data)

df['pos_tags']=df['Sentences'].apply(lambda x: pos_tag(word_tokenize(x)))

df

"""TFidfVectorize kelimelerin hangi sıklıkla kullanıldığını gösterir"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

sentence = "NLP is magnificient field to study .thats why I like to study NLP.NLP is very essential topic in Data science.Data science is great"

tf_idfVectorizer = TfidfVectorizer()

tf_idfMatrix = tf_idfVectorizer.fit_transform([sentence])

tf_idfMatrix.toarray()

pd.DataFrame(tf_idfMatrix.toarray(),columns = tf_idfVectorizer.get_feature_names_out())

"""CountVectorizer hangi kelimeden kaç tane olduğunu sayar"""

import nltk
nltk.download('punkt')
from sklearn.feature_extraction.text import CountVectorizer

tokens

Vectorizer = CountVectorizer()

bow_respresentation = Vectorizer.fit_transform([sentences]).toarray()

bow_respresentation

"""SentimentIntensityAnalyzer duygu analizi yapar compound değerine bak  1'e yakın ise pozitif  - değerse zaten negatif"""

import nltk
import pandas as pd
nltk.download('punkt')
nltk.download('vader_lexicon')
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.sentiment import SentimentIntensityAnalyzer

sample_sentence = "I love u babe"

sid = SentimentIntensityAnalyzer()

sentiment_scores =  sid.polarity_scores(sample_sentence)

sentiment_scores

data = {"Sentences" : ["fuck u dude",
                       "I like u babe!",
                       "u are dumb af",
                       "what a stunning girl!",
                       "",
                       ]}

df = pd.DataFrame(data)

df

df['Sentiment Scores'] = df['Sentences'].apply(lambda x: sid.polarity_scores(x)['compound'])

df['Sentiment Scores']



import numpy as np
import pandas as pd
df = pd.read_csv("/content/gender_classifier.csv" , encoding = "latin1")

df.head(3)

df.isnull().sum()

df['profile_yn_gold']

data1 = pd.concat([df.gender,df.description] , axis = 1)

data1.dropna(inplace = True)

data1.gender = [1 if i == "female" else 0 for i in data1.gender ]

data1

import re

metin = re.sub("[^a-zA-Z]", " " , data1.description[9])

metin

harfler = metin.lower()

harfler

bol = harfler.split()

import nltk
nltk.download('punkt')
from nltk.stem import PorterStemmer

ps = PorterStemmer()

stop = nltk.download("stopwords")

from nltk.corpus import stopwords

metin = [ps.stem(i) for i in bol if not i in set(stopwords.words("english"))]

metin

metinson = " ".join(metin)

metinson

data1.reset_index(drop = True , inplace = True)

liste = []
for j in range(1000):
  metin=re.sub("[^a-zA-Z]"," " , data1.description[j])
  metin = metin.lower()
  metin.split()
  metin=[ps.stem(i) for i in metin if not i in set(stopwords.words("english"))]
  metinson = " ".join(metin)
  liste.append(metinson)

liste

import numpy as np
import pandas as pd
df = pd.read_csv("/content/Reviews.csv")

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

df['Text'].values[0]

example = df['Text'][50]
tokens = nltk.word_tokenize(example)

nltk.pos_tag(tokens)

from nltk.sentiment import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

sia.polarity_scores("I am so happy")

sia.polarity_scores(example)

from tqdm.notebook import tqdm

df.head(3)

text = "Millions of people across the UK and beyond have celebrated the coronation of King Charles III - a symbolic ceremony combining a religious service and pageantry. The ceremony was held at Westminster Abbey, with the King becoming the 40th reigning monarch to be crowned there since 1066. Queen Camilla was crowned alongside him before a huge parade back to Buckingham Palace. Here's how the day of splendour and formality, which featured customs dating back more than 1,000 years, unfolded."
text

import nltk
nltk.download('punkt')

from nltk.tokenize import sent_tokenize

st = sent_tokenize(text)
st[2]

import re

text = re.sub(r"[^a-zA-Z]" , " ",st[2])

text

from nltk.tokenize import word_tokenize

words = word_tokenize(text)
words

from nltk.corpus import stopwords
nltk.download('stopwords')

words = [i for i in words if i not in stopwords.words('english')]

print(words)

stopwords.words('turkish')

nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

stemmed = [PorterStemmer().stem(i) for i in words]
print(stemmed)

print(words)

lemmatized = [WordNetLemmatizer().lemmatize(i) for i in words]
print(lemmatized)

words2 = ['wait','waiting','study','studying','computers']

stemmed = [PorterStemmer().stem(i) for i in words2]
print("Stemming output :" ,stemmed)
lemmatized = [WordNetLemmatizer().lemmatize(i) for i in words2]
print("Lemmatized output :" , lemmatized)

from nltk import pos_tag
nltk.download('averaged_perceptron_tagger')

pos_tag(words)

from nltk import ne_chunk

nltk.download('words')

